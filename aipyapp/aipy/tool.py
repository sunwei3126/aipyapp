import inspect
from typing import Any, Callable, Dict, Optional, List 

from loguru import logger
from pydantic import create_model, ValidationError

# å‡½æ•°æ³¨å†Œè¡¨ï¼šfunc_name -> meta ä¿¡æ¯
LLM_FUNCTION_REGISTRY: Dict[str, Dict[str, Any]] = {}

class LLMCallError(Exception):
    """LLM è°ƒç”¨ç›¸å…³çš„åŸºç¡€å¼‚å¸¸"""
    pass

class FunctionNotFoundError(LLMCallError):
    """å‡½æ•°/æ–¹æ³•æœªæ‰¾åˆ°å¼‚å¸¸"""
    pass

class ParameterValidationError(LLMCallError):
    """å‚æ•°éªŒè¯å¼‚å¸¸"""
    pass

def llm_callable(group: str = "default") -> Callable:
    def decorator(fn: Callable) -> Callable:
        sig = inspect.signature(fn)

        fields = {}
        for name, param in sig.parameters.items():
            annotation = param.annotation if param.annotation != inspect.Parameter.empty else Any
            default = param.default if param.default != inspect.Parameter.empty else ...
            fields[name] = (annotation, default)

        ParamModel = create_model(f"{fn.__name__}_Params", **fields)

        LLM_FUNCTION_REGISTRY[fn.__name__] = {
            "func": fn,
            "group": group,
            "signature": str(sig),
            "doc": inspect.getdoc(fn) or "",
            "param_model": ParamModel
        }

        return fn
    return decorator


def llm_call(func_name: str, **kwargs) -> Any:
    entry = LLM_FUNCTION_REGISTRY.get(func_name)
    if not entry:
        logger.error(f"è°ƒç”¨æœªæ³¨å†Œå‡½æ•°ï¼š{func_name}")
        raise FunctionNotFoundError(f"å‡½æ•° '{func_name}' æœªæ³¨å†Œã€‚")

    fn = entry["func"]
    ParamModel = entry.get("param_model")

    try:
        parsed = ParamModel(**kwargs)
        logger.info(f"è°ƒç”¨å‡½æ•° {func_name}ï¼Œå‚æ•°: {parsed.model_dump()}")
        return fn(**parsed.model_dump())

    except ValidationError as e:
        logger.error(f"å‡½æ•° '{func_name}' å‚æ•°æ ¡éªŒå¤±è´¥:\n{e}")
        raise ParameterValidationError(f"å‚æ•°æ ¡éªŒå¤±è´¥ï¼š\n{e}")


def get_llm_tools_description(group: Optional[str] = None, as_text: bool = False) -> str | Dict[str, Dict[str, str]]:
    tools_info: Dict[str, Dict[str, str]] = {}

    for name, meta in LLM_FUNCTION_REGISTRY.items():
        if group is None or meta["group"] == group:
            tools_info[name] = {
                "signature": meta["signature"],
                "doc": meta["doc"],
                "group": meta["group"]
            }

    if as_text:
        grouped: Dict[str, list[str]] = {}
        for name, meta in tools_info.items():
            grp = meta["group"]
            grouped.setdefault(grp, []).append(
                f"- {name}{meta['signature']}\n  {meta['doc']}"
                if meta["doc"] else f"- {name}{meta['signature']}"
            )
        lines = ["ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‡½æ•°ï¼ˆé€šè¿‡ llm_call(\"å‡½æ•°å\", å‚æ•°=å€¼) è°ƒç”¨ï¼‰ï¼š"]
        for group_name, func_lines in grouped.items():
            lines.append(f"\nğŸ“‚ {group_name}ï¼š")
            lines.extend(func_lines)
        return "\n".join(lines)

    return tools_info


def get_llm_tools_json_schema(group: Optional[str] = None) -> List[dict]:
    tools = []
    for name, meta in LLM_FUNCTION_REGISTRY.items():
        if group and meta["group"] != group:
            continue
        model = meta.get("param_model")
        if model is None:
            continue
        schema = model.model_json_schema()
        tools.append({
            "name": name,
            "description": meta["doc"],
            "parameters": schema
        })
    return tools


def get_llm_tools_multimodal_prompt(group: Optional[str] = None) -> str:
    tools = get_llm_tools_json_schema(group)
    lines = [
        "ä½ å¯ä»¥è°ƒç”¨ä»¥ä¸‹å‡½æ•°ã€‚æ¯ä¸ªå‡½æ•°åŒ…å«ï¼š",
        "- å‡½æ•°è¯´æ˜ï¼ˆè‡ªç„¶è¯­è¨€ï¼Œä¾›ä½ ç†è§£ç”¨é€”ï¼‰",
        "- å‚æ•°æ ¼å¼ï¼ˆJSON Schema æ ¼å¼ï¼Œä¾›ä½ æŒ‰è§„èŒƒç”Ÿæˆå‚æ•°ï¼‰\n"
    ]
    for tool in tools:
        lines.append(f"å‡½æ•°åï¼š{tool['name']}")
        lines.append(f"åŠŸèƒ½ï¼š{tool['description']}")
        lines.append("å‚æ•°è¦æ±‚ï¼ˆJSON Schemaï¼‰ï¼š")
        import json
        schema_json = json.dumps(tool["parameters"], indent=2, ensure_ascii=False)
        lines.append(schema_json)
        lines.append("-" * 30)
    return "\n".join(lines)

def register_instance_method(obj: Any, method_name: str, group: str = "default"):
    method = getattr(obj, method_name, None)
    if method is None:
        raise FunctionNotFoundError(f"å¯¹è±¡ {obj} æ²¡æœ‰æ–¹æ³• '{method_name}'")

    sig = inspect.signature(method)
    doc = inspect.getdoc(method) or ""

    fields = {}
    for name, param in sig.parameters.items():
        if name == "self":
            continue
        annotation = param.annotation if param.annotation != inspect.Parameter.empty else Any
        default = param.default if param.default != inspect.Parameter.empty else ...
        fields[name] = (annotation, default)

    ParamModel = create_model(f"{method_name}_Params", **fields)

    def wrapper(**kwargs):
        return method(**kwargs)

    wrapper.__name__ = method_name
    wrapper.__doc__ = doc

    LLM_FUNCTION_REGISTRY[method_name] = {
        "func": wrapper,
        "group": group,
        "signature": str(sig),
        "doc": doc,
        "param_model": ParamModel
    }

    return wrapper


def register_instance_methods(obj: Any, methods: List[str], group: str = "default"):
    for method in methods:
        register_instance_method(obj, method, group)


if __name__ == "__main__":
    # é¡¶å±‚å‡½æ•°
    @llm_callable(group="math")
    def add(x: int, y: int = 0) -> int:
        """è¿”å›ä¸¤ä¸ªæ•´æ•°çš„å’Œ"""
        return x + y

    assert llm_call("add", x="3", y=5) == 8

    # ç¼ºçœå‚æ•°
    assert llm_call("add", x=2) == 2

    # ç±»å‹é”™è¯¯
    try:
        llm_call("add", x="foo", y="bar")
    except ParameterValidationError as e:
        print("âœ… æ•è·ç±»å‹é”™è¯¯:", e)

    # å®ä¾‹æ–¹æ³•æ³¨å†Œ
    class Greeter:
        def greet(self, name: str, title: str = "æœ‹å‹") -> str:
            """æ‰“æ‹›å‘¼"""
            return f"ä½ å¥½ï¼Œ{title}{name}"

    g = Greeter()
    register_instance_method(g, "greet", group="social")
    assert llm_call("greet", name="å°æ˜") == "ä½ å¥½ï¼Œæœ‹å‹å°æ˜"
    assert llm_call("greet", name="æè€å¸ˆ", title="å°Šæ•¬çš„") == "ä½ å¥½ï¼Œå°Šæ•¬çš„æè€å¸ˆ"

    # æç¤ºä¿¡æ¯è¾“å‡º
    print("\n[è‡ªç„¶è¯­è¨€æç¤º]\n")
    print(get_llm_tools_description(as_text=True))

    print("\n[JSON Schema æç¤º]\n")
    import json
    print(json.dumps(get_llm_tools_json_schema(), indent=2, ensure_ascii=False))

    print("\n[å¤šæ¨¡æ€æç¤º]\n")
    print(get_llm_tools_multimodal_prompt())
